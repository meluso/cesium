#!/bin/bash

#-------------------  Begin SLURM preamble  -------------------------#
#SBATCH --job-name=cesium_rename_exec003
#SBATCH --partition=bluemoon
#SBATCH --ntasks=1
#SBATCH --mem-per-cpu=4000mb
#SBATCH --time=0-12:00:00
#SBATCH --mail-type=ALL
#SBATCH --output=/gpfs1/home/j/m/%u/cesium/logs/exec001/%x_%j.log
#-------------------   End SLURM preamble   -------------------------#

# Specify data directory
JOBDIR=/gpfs2/scratch/jmeluso/cesium/data/exec003
EXECNUM=3

# Make job directory
if [ ! -d $JOBDIR ] ; then
	mkdir $JOBDIR
fi

# Echo some useful and interesting information
echo "  running host:    ${SLURMD_NODENAME}"
echo "  assigned nodes:  ${SLURM_JOB_NODELIST}"
echo "  partition used:  ${SLURM_JOB_PARTITION}"
echo "  jobid:           ${SLURM_JOBID}"
echo ""

# Load python module
spack load python@3.7.7
spack load py-numpy@1.18.4
spack load py-networkx@2.4

# Change filenames for execution
python get_data.py $JOBDIR $EXECNUM


# to submit 100 jobs, call the file submit_loop.sh
# to run live, use the following command:
#   srun --partition=bluemoon --ntasks=1 --mem=4G --time=4:00:00 --pty /bin/bash